buildscript {
    ext {
        es_version = '7.8.1'
        distribution = 'oss-zip'
        es_group = "org.elasticsearch"
    }

    repositories {
        mavenCentral()
        jcenter()
    }

    dependencies {
        classpath "${es_group}.gradle:build-tools:${es_version}"
    }
}

apply plugin: 'java'
apply plugin: 'elasticsearch.esplugin'
apply plugin: 'elasticsearch.testclusters'
apply plugin: 'idea'

ext {
    projectSubstitutions = [:]
    licenseFile = rootProject.file('LICENSE.txt')
    noticeFile = rootProject.file('NOTICE.txt')
}

group 'com.amazonaws.elasticsearch'
version es_version

sourceCompatibility = 1.9

repositories {
    mavenCentral()
}

dependencies {
    testCompile ('junit:junit:4.12') {
        exclude group: 'org.hamcrest'
    }
    compileOnly ("org.elasticsearch.plugin:transport-netty4-client:7.8.1")
    compileOnly "org.elasticsearch:elasticsearch:${es_version}"
    compileOnly "com.amazon.opendistroforelasticsearch:opendistro-job-scheduler-spi:1.9.0.0"
}
compileTestJava {
    classpath = classpath.filter{ File file ->
        !file.name.equals( "hamcrest-core-1.3.jar" )
    }
}
esplugin {
    name 'async_search_plugin'
    description "Provides support for async search"
    classname 'com.amazon.opendistroforelasticsearch.search.async.plugin.AsyncSearchPlugin'
    //extendedPlugins = ['opendistro-job-scheduler']
}

licenseHeaders.enabled = true
dependencyLicenses.enabled = false
thirdPartyAudit.enabled = false
validateNebulaPom.enabled = false

def es_tmp_dir = rootProject.file('build/private/es_tmp').absoluteFile
es_tmp_dir.mkdirs()

afterEvaluate {
    testClusters.integTest.nodes.each { node ->
        def plugins = node.plugins
        def firstPlugin = plugins.get(0)
        plugins.remove(0)
        plugins.add(firstPlugin)
    }
}

test {
    systemProperty 'tests.security.manager', 'false'
    systemProperty 'es.set.netty.runtime.available.processors', 'false'

}

File repo = file("$buildDir/testclusters/repo")
def _numNodes = findProperty('numNodes') as Integer ?: 1

testClusters.integTest {
    testDistribution = "OSS"


}

//TODO integTest.runner {
//    systemProperty 'tests.security.manager', 'false'
//    systemProperty 'es.set.netty.runtime.available.processors', 'false'
//}


integTest {
    runner {
        systemProperty 'tests.security.manager', 'false'
        systemProperty 'java.io.tmpdir', es_tmp_dir.absolutePath

        systemProperty "https", System.getProperty("https")
        systemProperty "user", System.getProperty("user")
        systemProperty "password", System.getProperty("password")

        // The 'doFirst' delays till execution time.
        doFirst {
            // Tell the test JVM if the cluster JVM is running under a debugger so that tests can
            // use longer timeouts for requests.
            def isDebuggingCluster = getDebug() || System.getProperty("test.debug") != null
            systemProperty 'cluster.debug', isDebuggingCluster
            // Set number of nodes system property to be used in tests
            systemProperty 'cluster.number_of_nodes', "${_numNodes}"
            // There seems to be an issue when running multi node run or integ tasks with unicast_hosts
            // not being written, the waitForAllConditions ensures it's written
            getClusters().forEach { cluster ->
                cluster.waitForAllConditions()
            }
        }

        // The --debug-jvm command-line option makes the cluster debuggable; this makes the tests debuggable
        if (System.getProperty("test.debug") == null) {
            jvmArgs '-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=*:5005'
        }
    }
}

testClusters.integTest {
    testDistribution = "OSS"
    // Cluster shrink exception thrown if we try to set numberOfNodes to 1, so only apply if > 1
    if (_numNodes > 1) numberOfNodes = _numNodes
    // When running integration tests it doesn't forward the --debug-jvm to the cluster anymore
    // i.e. we have to use a custom property to flag when we want to debug elasticsearch JVM
    // since we also support multi node integration tests we increase debugPort per node
    if (System.getProperty("es.debug") != null) {
        def debugPort = 5005
        nodes.forEach { node ->
            node.jvmArgs("-agentlib:jdwp=transport=dt_socket,server=n,suspend=y,address=*:${debugPort}")
            debugPort += 1
        }
    }
//    plugin(fileTree("src/test/resources/job-scheduler").getSingleFile())

    // As of ES 7.7.0 the opendistro-anomaly-detection plugin is being added to the list of plugins for the testCluster during build before
    // the opendistro-job-scheduler plugin, which is causing build failures. From the stack trace, this looks like a bug.
    //
    // Exception in thread "main" java.lang.IllegalArgumentException: Missing plugin [opendistro-job-scheduler], dependency of [opendistro-anomaly-detection]
    //       at org.elasticsearch.plugins.PluginsService.addSortedBundle(PluginsService.java:452)
    //
    // One explanation is that ES build script sort plugins according to the natural ordering of their names.
    // opendistro-anomaly-detection comes before opendistro-job-scheduler.
    //
    // The following is a comparison of different plugin installation order:
    // Before 7.7:
    // ./bin/elasticsearch-plugin install --batch file:opendistro-anomaly-detection.zip file:opendistro-job-scheduler.zip
    //
    // After 7.7:
    // ./bin/elasticsearch-plugin install --batch file:opendistro-job-scheduler.zip file:opendistro-anomaly-detection.zip
    //
    // A temporary hack is to reorder the plugins list after evaluation but prior to task execution when the plugins are installed.
    nodes.each { node ->
        def plugins = node.plugins
        def firstPlugin = plugins.get(0)
        plugins.remove(0)
        plugins.add(firstPlugin)
    }
}

run {
    doFirst {
        // There seems to be an issue when running multi node run or integ tasks with unicast_hosts
        // not being written, the waitForAllConditions ensures it's written
        getClusters().forEach { cluster ->
            cluster.waitForAllConditions()
        }
    }
}

run {
    doFirst {
        // There seems to be an issue when running multi node run or integ tasks with unicast_hosts
        // not being written, the waitForAllConditions ensures it's written
        getClusters().forEach { cluster ->
            cluster.waitForAllConditions()
        }
    }
}
